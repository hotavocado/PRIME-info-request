record_id,"1. What elements do you believe are most essential for inclusion in such a movie stimulus (e.g., conspecific individuals, faces, bodies, social and non-social, interactions, vocalizations, other sounds...)?"
1,Variety to be more naturalistic
2,"faces, objects, complex visual stimuli"
3,NA
4,N/A
5,"- conspecific and human  faces / bodies   - conspecific and human voices (multiple call types, for human both speech and non speech)  - natural sounds (artificial and heterospecific sounds)  - tones with different frequencies"
6,"Auditory metronome ( brief tones (500Hz, 40 ms, 65 dBs) with an interstimulus interval od 550 or 850 ms."
7,N/A
8,face
9,"face, object, motion, social, high and low arousal movies"
10,N/A
11,N/A
12,"Shapes would be an obvious choice to map the visual cortex, and faces with different expressions would provide information about emotion processing. The inclusion of specific sounds or sound cues could give insight into auditory information processing."
13,"Movies, sounds and stimuli with social and non social features"
14,"It was quite a laborious process to tailor-making footages. I suggest we can build a library/database by gathering all these footages across different labs. Each lab has to provide a read-me txt file. Then, for parties who are interested we can decide/shortlist a subset for running experiments. The advantage with this approach is that we can make use of what we already have in store.    For example, we have put up 2,000 4-6 second long videos up here: http://118.126.113.127/ (just put in random values to get started to view them). These footages are made with the following constraints: 1) the clip must contain a continuous flow of depiction of events (i.e., no scene transition); 2) at least one living creature must be included; 3a) at least one of the animals contained must be in obvious motion; 3b) the trajectories of these motion must be uni-directional (i.e., no back and forth motion of the same subject); 4) clips with snakes were discarded. Half of the clips were with primate content including various species of monkeys, great apes, bonobos, and so on, while the other half with non-primate content including other kinds of mammals, and other phyla such as birds, fish, reptiles, and insects.   I can share these videos with interested parties and we can see what we can do from there on."
15,"conspecific, faces, vocalizations"
16,NA
17,N/A
18,"Conspesifics that are moving and vocalizing, where you can see the faces. Ideally, it would be nice if the movie included conspecifics in different contexts."
19,NA
20,interactions and social and non-social
21,Hand actions/manipulation of objects
22,"Faces, Objects, moving dots"
23,Maybe a set of localizers could be considered (Each of them being designed to answer a specific question)  rather than a broad one that for instance will target social vs rest.
24,"I study chemosensory and interoceptive functional processes using paradigms of stimulus application, for such a localizer has not been well established."
25,NA
26,N/A
27,Our brains come in jars... no paradigms possible
28,"Our brains come in jars. post mortem, no paradigms possible"
29,conspecifics (faces+bodies) engaged in social interactions  conspecifics (faces+bodies) engaged in non-social interactions: e.g. moving around / foraging / eating / resting etc.   vocalization  high value non-social items: e.g. food / high value foraging places (inferred from context)
30,- 3 kinds of paradigm inside: event-related ; simple block design & Resting State  - at least 2 types of stimulation: visual (retinotopy for instance) and auditory (tonotopy for example).  - use same anesthetic if under sedation
31,N/A
32,"I have a number of videos that could be of use for this type of localizer from Russ and Leopold 2015.  However to expand on those videos, which were not designed as a localizer per se, I would place a greater focus on fluctuations in faces, body parts, colors, and motion.  Additionally, I would try to integrate vocalizations and other auditory components better.  All of this is going to depend on the length of the video as well, with my original movies face patches were best localized with at least 15 minutes of data, though with a more targeted movie it probably could be done in 8-10."
33,"Mainly N/A, in addition to the above, retinotopic mapping localizer."
34,NA
35,NA
36,N/A
37,I am considering about this topic.
38,"Object database, Face database, Social grooming pictures, Conditioned reward stimuli"
39,"A controlled mix of social and non-social, appetizing, threatening, and neutral close and distant scenes would be one option. I mentioned during the online meeting the work of Liza Bliss-Moreau (UC Davis) who used 300 videos (or 600?) of natural scenes (30 sec each), all rated by ethologists for their 'emotional' content. There is also the more recent work of Winrich Freiwald.  However, the social/emotional content might complicate interpretation and some may want to suggest rather basic and well characterized dynamic visual stimuli used to study properties of the visual system."
40,N/A
41,"Vocalizations, different oro-facial movements, faces"
42,"individuals, social interactions, vocalizations"
43,NA
44,NA
45,"NA"
46,"orientation  motion  faces  actions (grasping, etc)  (social) interactions    I'm focusing on vision here as sound would in many cases (at least in our case) be tricky to implement just for this purpose."
47,N/A
